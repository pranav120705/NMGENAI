{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOiW3zDKMOWh7PbC74Nkv/c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranav120705/NMGENAI/blob/main/GenAIEX5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WIKIPEDIA ChatBot"
      ],
      "metadata": {
        "id": "SURsW3dT3PG_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_dXVX36aKYJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's dive deeper into the theoretical concepts behind this code.\n",
        "\n",
        "**1. Retrieval Augmented Generation (RAG):**\n",
        "\n",
        "* **Core Idea:** RAG addresses the limitations of Large Language Models (LLMs) by grounding their knowledge in external data sources. LLMs are trained on vast datasets, but their knowledge is static and may become outdated. RAG allows them to access and use real-time or domain-specific information.\n",
        "* **Process:**\n",
        "    * **Retrieval:** When a user asks a question, the system first retrieves relevant information from an external database (Wikipedia in this case).\n",
        "    * **Augmentation:** The retrieved information is then provided as context to the LLM.\n",
        "    * **Generation:** The LLM generates a response based on both its pre-trained knowledge and the retrieved context.\n",
        "* **Benefits:**\n",
        "    * Improved accuracy and relevance of responses.\n",
        "    * Ability to access up-to-date information.\n",
        "    * Reduced hallucination (generating false information).\n",
        "    * Increased transparency (source documents are provided).\n",
        "\n",
        "**2. Vector Embeddings and Similarity Search:**\n",
        "\n",
        "* **Vector Embeddings:**\n",
        "    * Text is transformed into numerical vectors that capture its semantic meaning.\n",
        "    * Words or phrases with similar meanings have vectors that are close to each other in vector space.\n",
        "    * Google's `GoogleGenerativeAIEmbeddings` model is used to create these vectors.\n",
        "* **Similarity Search:**\n",
        "    * The user's question is also converted into a vector embedding.\n",
        "    * A similarity search algorithm (implemented by FAISS) finds the text chunks in the database whose vectors are most similar to the question's vector.\n",
        "    * This retrieves the chunks that are most semantically relevant to the question.\n",
        "* **FAISS (Facebook AI Similarity Search):**\n",
        "    * A library optimized for fast and efficient similarity search in high-dimensional vector spaces.\n",
        "    * It enables the system to quickly find the most relevant text chunks from a large database.\n",
        "\n",
        "**3. Large Language Models (LLMs):**\n",
        "\n",
        "* **Generative Models:** LLMs are trained to generate text that is statistically similar to the text they have been trained on.\n",
        "* **Contextual Understanding:** They can understand the context of a conversation or query and generate responses that are relevant to that context.\n",
        "* **Gemini Pro:**\n",
        "    * In this code, Google's Gemini Pro model is used for text generation.\n",
        "    * It takes the retrieved text from Wikipedia as context and uses it to formulate an answer.\n",
        "* **Limitations:**\n",
        "    * LLMs can sometimes generate incorrect or nonsensical information (hallucinations).\n",
        "    * Their knowledge is limited to the data they were trained on.\n",
        "    * This is where RAG helps to mitigate those limitations.\n",
        "\n",
        "**4. LangChain:**\n",
        "\n",
        "* **Framework for LLM Applications:** LangChain provides tools and abstractions for building applications that use LLMs.\n",
        "* **Components:**\n",
        "    * **Document Loaders:** Load data from various sources (e.g., Wikipedia, PDFs, web pages).\n",
        "    * **Text Splitters:** Divide large documents into smaller chunks.\n",
        "    * **Vector Stores:** Store and retrieve vector embeddings.\n",
        "    * **Chains:** Combine multiple LLM calls and other components to perform complex tasks.\n",
        "* **Abstraction:** LangChain simplifies the process of building RAG systems by providing reusable components and a consistent API.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "This code leverages the power of LLMs by combining them with a retrieval system. Vector embeddings and similarity search enable efficient retrieval of relevant information from Wikipedia. LangChain provides the framework for orchestrating the entire process. The RAG approach allows the LLM to provide more accurate and contextually relevant answers by grounding its knowledge in external data.\n"
      ],
      "metadata": {
        "id": "qlB-KLuSNPLr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn2vTOV010RP",
        "outputId": "09b43b31-da15-4f9b-ef33-487964f2afed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question (or type 'exit'): what is space?\n",
            "Question: what is space?\n",
            "Answer: Space, or outer space, is the expanse beyond Earth's atmosphere and between celestial bodies.  It's a near-perfect vacuum containing very low densities of particles, mostly hydrogen and helium plasma. It also contains electromagnetic radiation, cosmic rays, neutrinos, magnetic fields, and dust.  The baseline temperature is extremely cold, at 2.7 kelvins (−270 °C; −455 °F), set by background radiation from the Big Bang.\n",
            "\n",
            "Source Documents:\n",
            "  - Outer space\n",
            "  - Outer space\n",
            "  - Outer space\n",
            "  - Outer space\n",
            "Ask a question (or type 'exit'): exit\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import langchain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "import wikipedia\n",
        "\n",
        "# Directly set the API key in Colab (less secure, but simpler for Colab)\n",
        "GOOGLE_API_KEY = \"AIzaSyC_NSrDX__MLPdb9mKqQW12wKAs8Xt68L0\"  # Replace with your actual API key\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"GOOGLE_API_KEY not set. Please replace 'YOUR_GOOGLE_API_KEY' with your key.\")\n",
        "\n",
        "def create_wikipedia_chatbot(question):\n",
        "    \"\"\"\n",
        "    Creates a chatbot that retrieves information from Wikipedia based on the question.\n",
        "\n",
        "    Args:\n",
        "        question (str): The user's question.\n",
        "\n",
        "    Returns:\n",
        "        RetrievalQA: A LangChain RetrievalQA chain, or None if an error occurred.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Use the question itself as the search query\n",
        "        try:\n",
        "            # Get the closest wikipedia page title.\n",
        "            best_guess = wikipedia.search(question)[0]\n",
        "            loader = WikipediaLoader(query=best_guess, load_max_docs=2)\n",
        "            documents = loader.load()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading Wikipedia data: {e}\")\n",
        "            return None\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "        db = FAISS.from_documents(texts, embeddings)\n",
        "        retriever = db.as_retriever()\n",
        "        llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n",
        "        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n",
        "        return qa_chain\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def ask_question(question):\n",
        "    \"\"\"\n",
        "    Asks a question to the chatbot and prints the answer.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to ask.\n",
        "    \"\"\"\n",
        "    qa_chain = create_wikipedia_chatbot(question)\n",
        "    if qa_chain:\n",
        "        try:\n",
        "            result = qa_chain({\"query\": question})\n",
        "            print(\"Question:\", question)\n",
        "            print(\"Answer:\", result[\"result\"])\n",
        "            print(\"\\nSource Documents:\")\n",
        "            for doc in result[\"source_documents\"]:\n",
        "                print(f\"  - {doc.metadata['title']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error asking question: {e}\")\n",
        "    else:\n",
        "        print(\"Chatbot not initialized.\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        user_question = input(\"Ask a question (or type 'exit'): \")\n",
        "        if user_question.lower() == \"exit\":\n",
        "            break\n",
        "        ask_question(user_question)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBIv-jrR9R5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "?"
      ],
      "metadata": {
        "id": "eEO__gzU3VRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pY-RMe-H3e_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FffZhIaX4aRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21JSe40N5zFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mm3md8nb7Bap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggk9sPpt9dQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d78277a-e75e-4779-b183-bcfb922a105f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question (or type 'exit'): hello \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2376b032fd62>:68: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = qa_chain({\"query\": question})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: hello \n",
            "Answer: Hello there! How can I help you today?\n",
            "\n",
            "Source Documents:\n",
            "  - Artificial general intelligence\n",
            "  - Artificial intelligence\n",
            "  - Artificial intelligence\n",
            "  - Artificial general intelligence\n",
            "Ask a question (or type 'exit'): sssssssss\n",
            "Question: sssssssss\n",
            "Answer: I'm not sure what you're asking. Can you please rephrase your question?\n",
            "\n",
            "Source Documents:\n",
            "  - Artificial intelligence\n",
            "  - Artificial general intelligence\n",
            "  - Artificial intelligence\n",
            "  - Artificial general intelligence\n",
            "Ask a question (or type 'exit'): why are you gay ??\n",
            "Question: why are you gay ??\n",
            "Answer: I'm not a person, so the concept of sexual orientation doesn't apply to me. I'm an AI, a computer program designed to provide information and complete tasks based on the data I was trained on.\n",
            "\n",
            "Source Documents:\n",
            "  - Artificial intelligence\n",
            "  - Artificial general intelligence\n",
            "  - Artificial general intelligence\n",
            "  - Artificial intelligence\n",
            "Ask a question (or type 'exit'): space\n",
            "Question: space\n",
            "Answer: This document does not contain the answer to this question.  It discusses artificial intelligence, its goals, and some of its sub-problems, but it does not discuss space or space exploration.\n",
            "\n",
            "Source Documents:\n",
            "  - Artificial intelligence\n",
            "  - Artificial intelligence\n",
            "  - Artificial general intelligence\n",
            "  - Artificial intelligence\n",
            "Ask a question (or type 'exit'): what is space?\n",
            "Question: what is space?\n",
            "Answer: This document does not contain an answer to this question. It focuses on artificial intelligence and does not discuss space or astronomy.\n",
            "\n",
            "Source Documents:\n",
            "  - Artificial intelligence\n",
            "  - Artificial general intelligence\n",
            "  - Artificial intelligence\n",
            "  - Artificial intelligence\n",
            "Ask a question (or type 'exit'): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai # Install the required package, langchain-google-genai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MFc7NimI_tpF",
        "outputId": "efe99701-a506-42df-dc93-62cc40171552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.50)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.4)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.3.23)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (4.13.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.17 langchain-google-genai-2.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "ceda83240b0a440db70404c9b618d6fc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community # Install the missing langchain-community package.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTCpqx2y_zbF",
        "outputId": "4e317deb-c122-4f96-a392-04c8bf9fe533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.50)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.22)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
            "  Downloading aiohttp-3.11.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.11.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.4.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.5/232.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, propcache, mypy-extensions, marshmallow, httpx-sse, frozenlist, aiohappyeyeballs, yarl, typing-inspect, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-community\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 dataclasses-json-0.6.7 frozenlist-1.5.0 httpx-sse-0.4.0 langchain-community-0.3.20 marshmallow-3.26.1 mypy-extensions-1.0.0 propcache-0.3.1 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0 yarl-1.18.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3mt4TkZ_-Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ZZkLZwdBnpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ooHzPDb-BnkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qNQGCsjyBndv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CZ0xxMOtBALt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxR3JpVSCaxF",
        "outputId": "1a198448-ed50-44e6-f75d-62b18f5c6cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question (or type 'exit'): what is space?\n",
            "No Wikipedia pages found for 'what'.\n",
            "Chatbot not initialized.\n",
            "Ask a question (or type 'exit'): space\n",
            "Question: space\n",
            "Answer: Space is a three-dimensional continuum containing positions and directions.  Classical physics often views it in three linear dimensions, while modern physics considers it, along with time, as part of a four-dimensional continuum called spacetime.  It's fundamental to understanding the physical universe, yet philosophers debate whether it's an entity, a relationship between entities, or part of a conceptual framework.  Historically, it's been conceived as both absolute (existing independently of matter) and relational (defined by relationships between objects).  Non-Euclidean geometries, where space is curved, have been developed, and Einstein's general relativity suggests space is indeed non-Euclidean around gravitational fields.\n",
            "\n",
            "Source Documents:\n",
            "  - Space\n",
            "  - Space\n",
            "  - Space\n",
            "  - Space\n",
            "Ask a question (or type 'exit'): exit \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: exit \n",
            "Answer: While the play's French title, *Huis clos*, literally translates to \"closed door,\" its more common English title is *No Exit*.  The play depicts three characters trapped in a room together for eternity, unable to leave.  Although at one point the door inexplicably opens, Garcin finds he cannot bring himself to exit. He realizes he is trapped not by a physical barrier, but by the presence and judgments of the other two characters.\n",
            "\n",
            "Source Documents:\n",
            "  - No Exit\n",
            "  - No Exit\n",
            "  - No Exit\n",
            "  - No Exit\n",
            "Ask a question (or type 'exit'): ecit\n",
            "Question: ecit\n",
            "Answer: ECIT (The Institute of Electronics, Communications and Information Technology) is located at Queen's University Belfast.  It focuses on research in areas such as cyber security, wireless communications, data science, and scalable computing.  It houses three research centers:\n",
            "\n",
            "* **CSIT (Centre for Secure Information Technologies):** The UK's largest university cyber security research lab.\n",
            "* **CWI (Centre for Wireless Innovation):** Highly ranked internationally for its research in telecommunication engineering.\n",
            "* **DSSC (Centre for Data Science and Scalable Computing)**\n",
            "\n",
            "ECIT has been quite successful, creating 2300 jobs in its first 15 years and receiving funding for expansion.  It's also part of the Belfast Regional City Deal.\n",
            "\n",
            "Source Documents:\n",
            "  - ECIT\n",
            "  - ECIT\n",
            "  - ECIT\n",
            "  - Cyprus\n",
            "Ask a question (or type 'exit'): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qhr8_wEzM4cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import langchain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "import wikipedia\n",
        "\n",
        "# Directly set the API key in Colab (less secure, but simpler for Colab)\n",
        "GOOGLE_API_KEY = \"AIzaSyC_NSrDX__MLPdb9mKqQW12wKAs8Xt68L0\"  # Replace with your actual API key\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"GOOGLE_API_KEY not set. Please replace 'YOUR_GOOGLE_API_KEY' with your key.\")\n",
        "\n",
        "def create_wikipedia_chatbot(question):\n",
        "    \"\"\"\n",
        "    Creates a chatbot that retrieves information from Wikipedia based on the question.\n",
        "\n",
        "    Args:\n",
        "        question (str): The user's question.\n",
        "\n",
        "    Returns:\n",
        "        RetrievalQA: A LangChain RetrievalQA chain, or None if an error occurred.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Use the question itself as the search query\n",
        "        try:\n",
        "            # Get the closest wikipedia page title.\n",
        "            best_guess = wikipedia.search(question)[0]\n",
        "            loader = WikipediaLoader(query=best_guess, load_max_docs=2)\n",
        "            documents = loader.load()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading Wikipedia data: {e}\")\n",
        "            return None\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "        db = FAISS.from_documents(texts, embeddings)\n",
        "        retriever = db.as_retriever()\n",
        "        llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n",
        "        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n",
        "        return qa_chain\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def ask_question(question):\n",
        "    \"\"\"\n",
        "    Asks a question to the chatbot and prints the answer.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to ask.\n",
        "    \"\"\"\n",
        "    qa_chain = create_wikipedia_chatbot(question)\n",
        "    if qa_chain:\n",
        "        try:\n",
        "            result = qa_chain({\"query\": question})\n",
        "            print(\"Question:\", question)\n",
        "            print(\"Answer:\", result[\"result\"])\n",
        "            print(\"\\nSource Documents:\")\n",
        "            for doc in result[\"source_documents\"]:\n",
        "                print(f\"  - {doc.metadata['title']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error asking question: {e}\")\n",
        "    else:\n",
        "        print(\"Chatbot not initialized.\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        user_question = input(\"Ask a question (or type 'exit'): \")\n",
        "        if user_question.lower() == \"exit\":\n",
        "            break\n",
        "        ask_question(user_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y558ukwDry-",
        "outputId": "e58af0ba-a9d6-429c-aefe-61fbbdb20ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question (or type 'exit'): what is relativity theory?\n",
            "Question: what is relativity theory?\n",
            "Answer: Relativity theory usually refers to two interrelated theories by Albert Einstein: special relativity and general relativity.  Special relativity, published in 1905, describes the relationship between space and time in the absence of gravity. General relativity, published in 1915, explains gravity as a geometric property of spacetime, expanding on special relativity and refining Newton's law of universal gravitation.\n",
            "\n",
            "Source Documents:\n",
            "  - Theory of relativity\n",
            "  - Theory of relativity\n",
            "  - Theory of relativity\n",
            "  - General relativity\n",
            "Ask a question (or type 'exit'): special relativity \n",
            "Question: special relativity \n",
            "Answer: Special relativity, a theory developed by Albert Einstein, describes the relationship between space and time. It's based on two postulates:\n",
            "\n",
            "1. **The laws of physics are the same for all observers in uniform motion** (i.e., no acceleration).  This is an extension of Galileo's principle of relativity.\n",
            "2. **The speed of light in a vacuum is the same for all observers**, regardless of the motion of the light source or the observer.\n",
            "\n",
            "These postulates have profound consequences, including:\n",
            "\n",
            "* **Relativity of simultaneity:** Events that are simultaneous for one observer may not be simultaneous for another observer in relative motion.\n",
            "* **Length contraction:**  The length of an object moving relative to an observer appears shorter in the direction of motion.\n",
            "* **Time dilation:** Time passes slower for a moving object relative to a stationary observer.\n",
            "* **Relativistic velocity addition:** Velocities don't add linearly at high speeds; the combined speed of two objects moving towards each other will always be less than the sum of their individual speeds.\n",
            "* **Mass-energy equivalence (E=mc²):**  Mass and energy are equivalent and interchangeable.\n",
            "\n",
            "Special relativity revolutionized our understanding of space, time, gravity, and the cosmos. It's a cornerstone of modern physics and has been experimentally verified numerous times.\n",
            "\n",
            "Source Documents:\n",
            "  - Theory of relativity\n",
            "  - Special relativity\n",
            "  - Theory of relativity\n",
            "  - Special relativity\n",
            "Ask a question (or type 'exit'): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-google-genai pypdf faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw-1uBvCEt4u",
        "outputId": "db27e7cd-b094-45be-f133-83b32be5c320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.50)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.23)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.17)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AwCmSrkpHxcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qbRyFmkrH2Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CE-kg6BtH2MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yp3dUvYPH2Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatBot using document"
      ],
      "metadata": {
        "id": "cz_05HY2H2pQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely, let's break down the code theoretically:\n",
        "\n",
        "**1. Core Concepts:**\n",
        "\n",
        "* **Retrieval Augmented Generation (RAG):**\n",
        "    * This code implements a basic RAG system. RAG is a technique where a language model's responses are enhanced by retrieving relevant information from an external knowledge source (in this case, a PDF document).\n",
        "    * The system first retrieves relevant chunks of text from the PDF based on the user's query and then uses a large language model (LLM) to generate an answer based on the retrieved information.\n",
        "\n",
        "* **LangChain:**\n",
        "    * LangChain is a framework that simplifies the development of applications powered by LLMs. It provides tools and abstractions for tasks like:\n",
        "        * Loading documents (e.g., PDFs).\n",
        "        * Splitting text into chunks.\n",
        "        * Creating vector embeddings.\n",
        "        * Storing and retrieving information from vector databases.\n",
        "        * Chaining together LLM calls.\n",
        "\n",
        "* **Vector Embeddings:**\n",
        "    * Vector embeddings are numerical representations of text that capture the semantic meaning of the text.\n",
        "    * The `GoogleGenerativeAIEmbeddings` model is used to create these embeddings.\n",
        "    * By comparing the embeddings of the user's question with the embeddings of the PDF's text chunks, the system can identify the most relevant chunks.\n",
        "\n",
        "* **FAISS (Facebook AI Similarity Search):**\n",
        "    * FAISS is a library for efficient similarity search and clustering of dense vectors.\n",
        "    * It's used as a vector database to store the embeddings of the PDF's text chunks.\n",
        "    * FAISS allows for fast retrieval of the most similar text chunks to the user's question.\n",
        "\n",
        "* **Large Language Model (LLM):**\n",
        "    * The `ChatGoogleGenerativeAI` model (specifically, \"models/gemini-1.5-pro\") is used to generate the final answer to the user's question.\n",
        "    * The LLM takes the retrieved text chunks from the PDF as context and generates a coherent and informative response.\n",
        "\n",
        "**2. Code Breakdown:**\n",
        "\n",
        "* **Importing Libraries:**\n",
        "    * The code imports necessary libraries from `langchain`, `langchain_google_genai`, and `langchain_community`.\n",
        "\n",
        "* **Setting API Key:**\n",
        "    * The `GOOGLE_API_KEY` is set. This is essential for accessing the Google Generative AI models.\n",
        "\n",
        "* **`create_pdf_chatbot` Function:**\n",
        "    * **Document Loading:** `PyPDFLoader` loads the PDF document and extracts its text content.\n",
        "    * **Text Splitting:** `RecursiveCharacterTextSplitter` divides the text into smaller chunks to improve retrieval accuracy.\n",
        "    * **Embedding Creation:** `GoogleGenerativeAIEmbeddings` generates vector embeddings for each text chunk.\n",
        "    * **Vector Database Creation:** `FAISS.from_documents` creates a FAISS vector database from the embeddings.\n",
        "    * **Retriever Creation:** `db.as_retriever()` creates a retriever object that can be used to retrieve relevant text chunks from the database.\n",
        "    * **LLM Initialization:** `ChatGoogleGenerativeAI` initializes the LLM.\n",
        "    * **QA Chain Creation:** `RetrievalQA.from_chain_type` creates a retrieval-based question-answering chain that combines the retriever and the LLM.\n",
        "    * **Error Handling:** The `try...except` block handles potential errors during the process.\n",
        "\n",
        "* **`ask_question_pdf` Function:**\n",
        "    * This function takes the PDF path and the user's question as input.\n",
        "    * It calls `create_pdf_chatbot` to initialize the QA chain.\n",
        "    * It then uses the QA chain to answer the question and prints the answer along with the source page numbers from the PDF.\n",
        "    * It also contains error handling.\n",
        "\n",
        "* **Example Usage:**\n",
        "    * The `if __name__ == \"__main__\":` block executes when the script is run.\n",
        "    * It prompts the user to enter the path to their PDF file.\n",
        "    * It then enters a loop that prompts the user to ask questions.\n",
        "    * The `ask_question_pdf` function is called to process each question.\n",
        "    * It also contains a check to make sure the pdf file exists.\n",
        "\n",
        "**In essence, the code automates the process of:**\n",
        "\n",
        "1.  Reading a PDF.\n",
        "2.  Preparing the PDF's content for efficient search.\n",
        "3.  Finding relevant information within the PDF based on a question.\n",
        "4.  Using an LLM to formulate a human-readable answer.\n",
        "5.  Providing the answer and the source page from the pdf.\n"
      ],
      "metadata": {
        "id": "sVCuts1qMvaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import langchain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Directly set the API key in Colab (less secure, but simpler for Colab)\n",
        "GOOGLE_API_KEY = \"AIzaSyC_NSrDX__MLPdb9mKqQW12wKAs8Xt68L0\"  # Replace with your actual API key\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"GOOGLE_API_KEY not set. Please replace 'YOUR_GOOGLE_API_KEY' with your key.\")\n",
        "\n",
        "def create_pdf_chatbot(pdf_path, question):\n",
        "    \"\"\"\n",
        "    Creates a chatbot that retrieves information from a PDF document based on the question.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The path to the PDF document.\n",
        "        question (str): The user's question.\n",
        "\n",
        "    Returns:\n",
        "        RetrievalQA: A LangChain RetrievalQA chain, or None if an error occurred.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        documents = loader.load()\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "        db = FAISS.from_documents(texts, embeddings)\n",
        "        retriever = db.as_retriever()\n",
        "        llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n",
        "        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n",
        "        return qa_chain\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def ask_question_pdf(pdf_path, question):\n",
        "    \"\"\"\n",
        "    Asks a question to the chatbot based on a PDF document and prints the answer.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The path to the PDF document.\n",
        "        question (str): The question to ask.\n",
        "    \"\"\"\n",
        "    qa_chain = create_pdf_chatbot(pdf_path, question)\n",
        "    if qa_chain:\n",
        "        try:\n",
        "            result = qa_chain({\"query\": question})\n",
        "            print(\"Question:\", question)\n",
        "            print(\"Answer:\", result[\"result\"])\n",
        "            print(\"\\nSource Documents:\")\n",
        "            for doc in result[\"source_documents\"]:\n",
        "                print(f\"  - Page {doc.metadata['page']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error asking question: {e}\")\n",
        "    else:\n",
        "        print(\"Chatbot not initialized.\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_file_path = input(\"Enter your_document.pdf: \")  # Replace with the path to your PDF file.\n",
        "    if not os.path.exists(pdf_file_path):\n",
        "        print(f\"Error: PDF file '{pdf_file_path}' not found.\")\n",
        "    else:\n",
        "\n",
        "        while True:\n",
        "            user_question = input(\"Ask a question (or type 'exit'): \")\n",
        "            if user_question.lower() == \"exit\":\n",
        "                break\n",
        "            ask_question_pdf(pdf_file_path, user_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "MR-Bp-4_H2HY",
        "outputId": "bbd7b62f-fdbf-47c7-c061-3aa89636ccda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your_document.pdf: AI for marketing org.pdf\n",
            "Ask a question (or type 'exit'): what is AI marketing?\n",
            "Question: what is AI marketing?\n",
            "Answer: AI marketing uses artificial intelligence to improve the customer journey, leading to better individual targeted campaigns and ROI.  It allows marketers to analyze data, predict trends, create more innovative and targeted advertisements, and automate tasks such as programmatic media bidding.  AI marketing also helps understand customer needs and expectations through integrated applications that track purchases and provide personalized marketing messages with suggestions and special offers.  This data-driven approach improves decision-making and provides a competitive advantage through system automation.\n",
            "\n",
            "Source Documents:\n",
            "  - Page 8\n",
            "  - Page 2\n",
            "  - Page 4\n",
            "  - Page 3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ec31abca7473>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask a question (or type 'exit'): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-google-genai faiss-cpu pubmed_parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK9iFGHXH2Ex",
        "outputId": "652406a3-1206-4a5d-ed78-0c320c00d3b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Collecting pubmed_parser\n",
            "  Downloading pubmed_parser-0.5.1-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.50)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.23)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.17)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Collecting lxml (from pubmed_parser)\n",
            "  Downloading lxml-5.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting unidecode (from pubmed_parser)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pubmed_parser) (1.17.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading pubmed_parser-0.5.1-py3-none-any.whl (56.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode, lxml, pubmed_parser\n",
            "Successfully installed lxml-5.3.1 pubmed_parser-0.5.1 unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xmltodict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxk1KFs-H2Ca",
        "outputId": "f3b5ab46-0db1-49f9-8b1e-4c3e60c50d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvVH8jmoQwY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPgqoZstQwSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down the theoretical concepts and the workflow of this code, which creates a health-related question-answering system using PubMed and Google's Gemini models.\n",
        "\n",
        "**1. Core Idea: Retrieval Augmented Generation (RAG) for Healthcare Information**\n",
        "\n",
        "* This code implements a RAG system tailored for healthcare queries.\n",
        "* It leverages PubMed, a vast repository of biomedical literature, as the external knowledge source.\n",
        "* The system aims to provide accurate and contextually relevant answers to health-related questions by retrieving information from PubMed articles and using a Large Language Model (LLM) to generate responses.\n",
        "\n",
        "**2. Key Components and Their Roles:**\n",
        "\n",
        "* **PubMedLoader:**\n",
        "    * This LangChain document loader retrieves relevant articles from PubMed based on the user's query.\n",
        "    * It acts as the retrieval mechanism, fetching biomedical information.\n",
        "* **RecursiveCharacterTextSplitter:**\n",
        "    * This text splitter breaks down the retrieved PubMed articles into smaller, manageable chunks.\n",
        "    * This is crucial because LLMs have limitations on the amount of text they can process at once.\n",
        "* **GoogleGenerativeAIEmbeddings (embedding-001):**\n",
        "    * This component generates vector embeddings for the text chunks from PubMed articles and the user's query.\n",
        "    * Vector embeddings are numerical representations of text that capture semantic meaning.\n",
        "    * `embedding-001` is the name of the model that creates those embeddings.\n",
        "* **FAISS (Facebook AI Similarity Search):**\n",
        "    * FAISS is a vector database that stores the embeddings of the PubMed text chunks.\n",
        "    * It efficiently performs similarity searches, finding the text chunks that are most relevant to the user's query.\n",
        "* **ChatGoogleGenerativeAI (gemini-1.5-pro):**\n",
        "    * This is the LLM that generates the final answer to the user's question.\n",
        "    * It takes the retrieved PubMed text chunks as context and formulates a coherent response.\n",
        "* **RetrievalQA:**\n",
        "    * This LangChain chain orchestrates the entire process, combining the retriever (FAISS and embeddings) and the LLM.\n",
        "    * It handles the retrieval of relevant information and the generation of the answer.\n",
        "\n",
        "**3. Workflow:**\n",
        "\n",
        "1.  **User Input:**\n",
        "    * The user enters a health-related question.\n",
        "2.  **PubMed Retrieval:**\n",
        "    * The `PubMedLoader` uses the user's question as a search query to retrieve relevant articles from PubMed.\n",
        "3.  **Text Processing:**\n",
        "    * The retrieved articles are split into text chunks using `RecursiveCharacterTextSplitter`.\n",
        "    * Vector embeddings are generated for the text chunks using `GoogleGenerativeAIEmbeddings`.\n",
        "4.  **Similarity Search:**\n",
        "    * The user's question is also converted into a vector embedding.\n",
        "    * FAISS performs a similarity search to find the PubMed text chunks that are most semantically similar to the question.\n",
        "5.  **Answer Generation:**\n",
        "    * The `RetrievalQA` chain provides the relevant text chunks as context to the `ChatGoogleGenerativeAI` model.\n",
        "    * The LLM generates a response based on the retrieved information.\n",
        "6.  **Output:**\n",
        "    * The code prints the user's question and the generated answer.\n",
        "\n",
        "**4. Theoretical Concepts:**\n",
        "\n",
        "* **Vector Space Models:**\n",
        "    * The use of vector embeddings and FAISS relies on the concept of vector space models, where text is represented as points in a high-dimensional space.\n",
        "    * Similarity between texts is determined by the distance between their corresponding vectors.\n",
        "* **Semantic Search:**\n",
        "    * The system performs semantic search, which aims to find information based on its meaning rather than just keyword matching.\n",
        "    * This allows the system to retrieve more relevant results, even if the user's question doesn't contain the exact keywords found in the PubMed articles.\n",
        "* **Large Language Models (LLMs):**\n",
        "    * LLMs are powerful generative models that can understand and generate human-like text.\n",
        "    * In this context, the LLM is used to synthesize the retrieved information and generate a concise and informative answer.\n",
        "* **LangChain as an Orchestration Framework:**\n",
        "    * Langchain, allows for the easy connection of different models, and databases, to allow for complex tasks to be completed.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "This code builds a sophisticated question-answering system that combines the power of PubMed's biomedical knowledge with the natural language processing capabilities of LLMs. It represents a practical application of RAG in the healthcare domain.\n"
      ],
      "metadata": {
        "id": "_KlIuu0SQww9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import langchain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.document_loaders import PubMedLoader  # Changed to PubMedLoader\n",
        "\n",
        "# Directly set the API key in Colab (less secure, but simpler for Colab)\n",
        "GOOGLE_API_KEY = \"AIzaSyC_NSrDX__MLPdb9mKqQW12wKAs8Xt68L0\"  # Replace with your actual API key\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"GOOGLE_API_KEY not set. Please replace 'YOUR_GOOGLE_API_KEY' with your key.\")\n",
        "\n",
        "def create_pubmed_chatbot(query):  # Changed to accept a query instead of pdf_path\n",
        "    \"\"\"\n",
        "    Creates a chatbot that retrieves information from PubMed based on the question.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query for PubMed.\n",
        "\n",
        "    Returns:\n",
        "        RetrievalQA: A LangChain RetrievalQA chain, or None if an error occurred.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        loader = PubMedLoader(query=query, load_max_docs=5)  # Adjusted load_max_docs\n",
        "        documents = loader.load()\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "        db = FAISS.from_documents(texts, embeddings)\n",
        "        retriever = db.as_retriever()\n",
        "        llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n",
        "        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n",
        "        return qa_chain\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def ask_question_pubmed(query):  # Changed to accept a query.\n",
        "    \"\"\"\n",
        "    Asks a question to the chatbot based on PubMed and prints the answer.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query for PubMed.\n",
        "    \"\"\"\n",
        "    qa_chain = create_pubmed_chatbot(query)\n",
        "    if qa_chain:\n",
        "        try:\n",
        "            result = qa_chain({\"query\": query})\n",
        "            print(\"Question:\", query)\n",
        "            print(\"Answer:\", result[\"result\"])\n",
        "          # changed from page to title\n",
        "        except Exception as e:\n",
        "            print(f\"Error asking question: {e}\")\n",
        "    else:\n",
        "        print(\"Chatbot not initialized.\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        user_question = input(\"Enter your health-related question (or type 'exit'): \")\n",
        "        if user_question.lower() == \"exit\":\n",
        "            break\n",
        "        ask_question_pubmed(user_question) # changed to user question."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq-J_E5ZH19A",
        "outputId": "941327b2-d33b-44c3-f0c6-a65eda8b2a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your health-related question (or type 'exit'): im feeling like fever\n",
            "Too Many Requests, waiting for 0.20 seconds...\n",
            "Too Many Requests, waiting for 0.40 seconds...\n",
            "Question: im feeling like fever\n",
            "Answer: This model is not able to give medical advice.  A medical expert, like a doctor, will be able to help you understand why you are feeling feverish and recommend treatment.\n",
            "Enter your health-related question (or type 'exit'): why it is occuring?\n",
            "Too Many Requests, waiting for 0.20 seconds...\n",
            "Too Many Requests, waiting for 0.40 seconds...\n",
            "Question: why it is occuring?\n",
            "Answer: This question is too broad.  I need more context. What is \"it\"? Please clarify what you're asking about.\n",
            "Enter your health-related question (or type 'exit'): why fever is occuring?\n",
            "Too Many Requests, waiting for 0.20 seconds...\n",
            "Too Many Requests, waiting for 0.40 seconds...\n",
            "Question: why fever is occuring?\n",
            "Answer: This document discusses two separate cases.  In the first case, fever is mentioned as a possible symptom of Acute Generalized Exanthematous Pustulosis (AGEP), sometimes associated with viral infection. In the second case, the child presented with \"subjective fever\" after abdominal trauma, which was later determined to be caused by a ruptured bladder.  Neither case definitively explains the underlying cause of fever in general.\n",
            "Enter your health-related question (or type 'exit'): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9iEJAyZ-H13w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLBRzpTYS2FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oCxAKiL4S2DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kJdPjTSES2A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yDcqb07GS2h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-google-genai faiss-cpu clip torch whisper pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIn1vCEsS1-P",
        "outputId": "e7f6d19e-1c07-469a-de64-4787545104de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: clip in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: whisper in /usr/local/lib/python3.11/dist-packages (1.1.10)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.49)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.19)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.17)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from whisper) (1.17.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1eOaIdtAS17n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQFemnL4H11A",
        "outputId": "7eea0ea4-feaf-4a35-afdd-857495d57472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.49)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.21)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.19)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.11.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.4.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.20 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured[audio]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHYMcyeATkFn",
        "outputId": "341960cd-f431-4413-bb63-045bdfd7a193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured[audio]\n",
            "  Downloading unstructured-0.17.2-py3-none-any.whl.metadata (24 kB)\n",
            "\u001b[33mWARNING: unstructured 0.17.2 does not provide the extra 'audio'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (1.2.0)\n",
            "Collecting python-magic (from unstructured[audio])\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (5.3.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (4.13.3)\n",
            "Collecting emoji (from unstructured[audio])\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (0.6.7)\n",
            "Collecting python-iso639 (from unstructured[audio])\n",
            "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured[audio])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (2.0.2)\n",
            "Collecting rapidfuzz (from unstructured[audio])\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting backoff (from unstructured[audio])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (4.13.0)\n",
            "Collecting unstructured-client (from unstructured[audio])\n",
            "  Downloading unstructured_client-0.32.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (1.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured[audio])\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured[audio]) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured[audio]) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured[audio]) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured[audio]) (0.9.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured[audio]) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured[audio]) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[audio]) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[audio]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[audio]) (2024.11.6)\n",
            "Collecting olefile (from python-oxmsg->unstructured[audio])\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[audio]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[audio]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[audio]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[audio]) (2025.1.31)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured[audio])\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[audio]) (43.0.3)\n",
            "Collecting eval-type-backport>=0.2.0 (from unstructured-client->unstructured[audio])\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[audio]) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[audio]) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[audio]) (2.11.0)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->unstructured[audio])\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[audio]) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[audio]) (1.0.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[audio]) (0.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client->unstructured[audio]) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[audio]) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[audio]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[audio]) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured[audio]) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->unstructured-client->unstructured[audio]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->unstructured-client->unstructured[audio]) (2.33.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[audio]) (1.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured[audio]) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[audio]) (1.3.1)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.17.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.32.1-py3-none-any.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.5/180.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=880144de011a6b7d7dbcfe9ad376bdb5cc87f917ebce4bf21d6efa284b5aa814\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: rapidfuzz, python-magic, python-iso639, pypdf, olefile, langdetect, eval-type-backport, emoji, backoff, aiofiles, python-oxmsg, unstructured-client, unstructured\n",
            "Successfully installed aiofiles-24.1.0 backoff-2.2.1 emoji-2.14.1 eval-type-backport-0.2.2 langdetect-1.0.9 olefile-0.47 pypdf-5.4.0 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.12.2 unstructured-0.17.2 unstructured-client-0.32.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FekVAjiYTtU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k34ILb4kUfNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RUGYBQ-dVBtu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}